#!/usr/bin/env python3
"""
Teste simples para o QuestionEvaluator
"""
import asyncio
import os
import sys
from pathlib import Path

# Adicionar o diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent))

from core.question_evaluator import QuestionEvaluator
from core.ai.service import AIService
from shared.config import AIProvider


async def test_question_evaluator():
    """Testa o QuestionEvaluator com dados de exemplo"""
    
    # Verificar se h√° API key configurada
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ùå OPENAI_API_KEY n√£o configurada")
        print("Configure a vari√°vel de ambiente OPENAI_API_KEY para testar")
        return
    
    try:
        # Configurar o servi√ßo de IA
        print("üîß Configurando AIService...")
        ai_service = AIService(
            provider=AIProvider.OPENAI,
            api_key=api_key
        )
        
        # Criar inst√¢ncia do avaliador
        print("üîß Criando QuestionEvaluator...")
        evaluator = QuestionEvaluator(ai_service)
        
        # Dados de teste
        job_data = {
            "title": "Desenvolvedor Python",
            "description": "Desenvolvedor Python para criar APIs e sistemas web",
            "requirements": "Python, FastAPI, SQL, experi√™ncia com desenvolvimento web"
        }
        
        question_responses = [
            {
                "question": "Como voc√™ implementaria uma API REST?",
                "answer": "Usaria FastAPI para criar endpoints, definir modelos Pydantic para valida√ß√£o, implementar autentica√ß√£o JWT e usar SQLAlchemy para banco de dados."
            },
            {
                "question": "Explique o que √© um ORM",
                "answer": "ORM (Object-Relational Mapping) √© uma t√©cnica que permite trabalhar com banco de dados usando objetos Python ao inv√©s de SQL direto, facilitando o desenvolvimento e manuten√ß√£o."
            }
        ]
        
        print("üöÄ Iniciando avalia√ß√£o das respostas...")
        print(f"Vaga: {job_data['title']}")
        print(f"N√∫mero de perguntas: {len(question_responses)}")
        
        # Avaliar as respostas
        evaluation = await evaluator.evaluate_question_responses(
            question_responses=question_responses,
            job_data=job_data,
            temperature=0.3
        )
        
        print("\n‚úÖ Avalia√ß√£o conclu√≠da!")
        print("=" * 50)
        print(f"Score geral: {evaluation['score']}/100")
        print(f"Feedback: {evaluation['feedback']}")
        
        if evaluation.get('improvement_areas'):
            print(f"\n√Åreas de melhoria:")
            for area in evaluation['improvement_areas']:
                print(f"  ‚Ä¢ {area}")
        
        if evaluation.get('details'):
            details = evaluation['details']
            print(f"\nDetalhes da avalia√ß√£o:")
            if 'relevance_score' in details:
                print(f"  ‚Ä¢ Relev√¢ncia: {details['relevance_score']}/100")
            if 'specificity_score' in details:
                print(f"  ‚Ä¢ Especificidade: {details['specificity_score']}/100")
            if 'alignment_score' in details:
                print(f"  ‚Ä¢ Alinhamento: {details['alignment_score']}/100")
        
        print(f"\nProvider: {evaluation.get('provider', 'N/A')}")
        print(f"Modelo: {evaluation.get('model', 'N/A')}")
        
    except Exception as e:
        print(f"‚ùå Erro durante o teste: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("üß™ Testando QuestionEvaluator")
    print("=" * 50)
    
    # Executar o teste
    asyncio.run(test_question_evaluator())
